## 1. 介绍

本文将向你展示一个使用虚拟线程的简单网络爬虫。网络爬虫将获取网页并从中提取新的URL进行爬网。我正在使用虚拟线程，因为它们的创建成本更低，而且我可以同时拥有更多虚拟线程。虚拟线程也使阻塞变得非常便宜；这不是问题，例如，当线程必须等待Web服务器的响应时。如果你想了解有关虚拟线程的更多信息，请参阅[这篇文章](https://davidvlijmincx.com/posts/create_virtual_threads_with_project_loom/)。

## 2. 使用虚拟线程构建抓取器

这个爬虫程序的想法是为每个URL设置一个虚拟线程，这样当一个线程在等待网页时被阻塞时，其他线程可以运行。在下面的代码中，你会看到整个网络爬虫类。在start方法中，我们有一个while循环，从双端队列获取URI并将其提交给ExecutorService。

虚拟线程使这个爬虫比其他使用旧(系统)线程的爬虫更特别。在第9行，我们有一个try包含两个执行程序服务的语句。我使用一种执行器服务来处理HttpClient发送的请求，另一种用于在HTTP响应中查找URL。由于[有序取消](https://davidvlijmincx.com/posts/loom/java_structured_concurrency/)，try语句中执行程序服务的顺序很重要。在关闭处理响应的之前，我们不能关闭HttpClient使用的executorService。

```java
public class WebCrawler {
    Pattern UrlRegex = Pattern.compile("[-a-zA-Z\\d@:%._+~#=]{1,256}\\.[a-zA-Z\\d()]{1,6}\\b([-a-zA-Z\\d()@:%_+.~#?&/=]*)");
    Set<URI> foundURIs = new HashSet<>();
    LinkedBlockingDeque<URI> deque = new LinkedBlockingDeque<>();

    public void start(URI startURI) {
        deque.add(startURI);

        try (ExecutorService httpClientExecutorService = Executors.newVirtualThreadPerTaskExecutor();
             ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {

            HttpClient client = HttpClient.newBuilder()
                  .followRedirects(HttpClient.Redirect.ALWAYS)
                  .connectTimeout(Duration.ofSeconds(1))
                  .executor(httpClientExecutorService)
                  .build();

            while (foundURIs.size() < 5) {
                try {
                    URI uri = deque.take();
                    System.out.println("uri = " + uri);
                    executor.submit(() -> crawl(uri, client));
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            }
        }
        System.out.println("foundURIs = " + foundURIs);
    }

    private void crawl(URI uri, HttpClient client) {

        var request = HttpRequest.newBuilder()
              .uri(uri)
              .GET()
              .build();

        try {
            var response = client.send(request, HttpResponse.BodyHandlers.ofString());

            UrlRegex.matcher(response.body())
                  .results()
                  .map(m -> m.group(0))
                  .map(s -> response.uri().resolve(s))
                  .forEach(s -> {
                      if (foundURIs.add(s)) {
                          deque.add(s);
                      }
                  });
        } catch (Exception e) {
            System.out.println("Failed to parse URI: " + uri);
        }
    }
}
```

要启动网络爬虫，你只需创建一个类实例并使用它可以使用的初始URL调用start()方法。

```java
WebCrawler webCrawler = new WebCrawler();
webCrawler.start(URI.create("https://www.davidvlijmincx.com/"));
```

## 3. 总结

在这篇文章中，我们研究了一个使用虚拟线程的简单网络爬虫。我们讨论了它的工作原理以及管理和创建线程的位置。我们还看到了一个案例，因为顺序取消，执行者的顺序是必不可少的。